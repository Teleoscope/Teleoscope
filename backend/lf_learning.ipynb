{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d979288c-7e14-4d92-b9de-6c2acc9571ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f9d1b9-d81c-4c83-9c9d-30d03eab859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from bson.objectid import ObjectId\n",
    "import gc\n",
    "import tasks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numba\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "import umap.plot\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for jupyter notebook widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac8027c-af0d-4bf3-8d06-176ced95da0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['20.220.215.35:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', authmechanism='SCRAM-SHA-256', connecttimeoutms=50000, serverselectiontimeoutms=50000, directconnection=True, replicaset='rs0'), 'aita')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to database\n",
    "db = utils.connect()\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe4861-8e82-4434-b536-fcf9541d0648",
   "metadata": {},
   "source": [
    "### Setup Human Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4448d3-9fca-44af-a4df-16718f686a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 groups from database\n"
     ]
    }
   ],
   "source": [
    "# hardcoded group id strings\n",
    "group_id_strings = ['63901a89e189962b660959cf', '63901a92931eeac91c9924a1', '63901a96e189962b660959d3']\n",
    "\n",
    "# convert to objectId's\n",
    "group_ids = [ObjectId(str(id)) for id in group_id_strings]\n",
    "\n",
    "# retrieve groups from database\n",
    "groups = list(db.groups.find({\"_id\":{\"$in\" : group_ids}}))\n",
    "print(\"Retrieved \" + str(len(groups)) + \" groups from database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8244c486-26d3-409d-b914-30eaec8a44a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hetv62', 'lwd55z', 'dhbdpv', 'eyj0sv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups[0]['history'][0]['included_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c27a74c-e029-40a5-b0ee-c69cd123b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1, 'textVector': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projection here to only include the fields we want\n",
    "projection = {'id': 1, 'textVector': 1}\n",
    "projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74e643-031e-4e66-a2d1-ba5c04f935df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54c336-714e-42b1-94b5-24fe2b2f707e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Save & Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415165c7-d80c-4728-b3e2-f6659abe0808",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using First Group's Teleoscope Ordering\n",
    "\n",
    "Change Raw cells below to Code if you need to reload document ids / vectors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eabfa6c-243a-4e36-ae65-f2af0e9a75a4",
   "metadata": {},
   "source": [
    "# default to ordering documents relative to first group's teleoscope\n",
    "teleoscope_oid = groups[0][\"teleoscope\"]\n",
    "teleoscope_oid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83972eb-2932-4801-9936-9353a94fc786",
   "metadata": {
    "tags": []
   },
   "source": [
    "teleoscope = db.teleoscopes.find_one({\"_id\": ObjectId(str(teleoscope_oid))})\n",
    "#teleoscope"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b3894a6-20b1-4156-9fb7-fc9b15382bc1",
   "metadata": {},
   "source": [
    "# saved as ordered_documents.npz\n",
    "# change to code cell if load is needed\n",
    "\n",
    "# get Teleoscope from GridFS\n",
    "all_ordered_documents = utils.gridfsDownload(db, \"teleoscopes\", ObjectId(str(teleoscope[\"history\"][0][\"ranked_document_ids\"])))\n",
    "\n",
    "# np.savez_compressed('all_ordered_documents', ord_docs=all_ordered_documents)\n",
    "# all_ordered_documents = np.load('all_ordered_documents.npz')['ord_docs']\n",
    "\n",
    "len(all_ordered_documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdab68b3-ba24-415c-a01b-7bf11fdddac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# grab only subset of the ordered documents\n",
    "limit = 10000\n",
    "# TODO: does this line generate an out of bounds access?\n",
    "ordered_documents = all_ordered_documents[0:limit]\n",
    "limit = min(limit, len(ordered_documents))\n",
    "limit"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ff88c0d-2f74-4434-8805-a5d8bf771a38",
   "metadata": {},
   "source": [
    "ordered_documents = all_ordered_documents\n",
    "limit = len(ordered_documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab1f7ac9-9c38-4273-bfb9-f2e0933df270",
   "metadata": {},
   "source": [
    "# cursor is a generator which means it yields a new doc one at a time\n",
    "cursor = db.documents.find(\n",
    "    # query\n",
    "    {\"id\":{\"$in\": [document[0] for document in ordered_documents]}},\n",
    "    projection=projection,\n",
    "    # batch size means number of documents at a time taken from MDB, no impact on iteration\n",
    "    batch_size=500\n",
    ")\n",
    "document_ids = []\n",
    "document_vectors = []\n",
    "\n",
    "# for large datasets, this will take a while. Would be better to find out whether the UMAP fns can \n",
    "# accept generators for lazy calculation\n",
    "for document in tqdm.tqdm(cursor, total=limit):\n",
    "    document_ids.append(document[\"id\"])\n",
    "    document_vectors.append(document[\"textVector\"])\n",
    "\n",
    "print(\"There are \" + str(len(document_ids)) + \" document ids.\")\n",
    "print(\"There are \" + str(len(document_vectors)) + \" document vectors.\")\n",
    "\n",
    "np.savez_compressed('teleo_order_docs', doc_ids=document_ids, doc_vecs=document_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975b6eb-51c0-408b-9232-5967aba84cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using All Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fd1390-d60c-4ae8-b139-248acdec6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacheClusteringData(db):\n",
    "    \"\"\"\n",
    "    Check to see if distance matrix and list of document ids is cached in ~/embeddings\n",
    "    \n",
    "    input:\n",
    "        db: mongoDB connection\n",
    "    output:\n",
    "        dm: distance matrix\n",
    "        ids: list of document ids\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    npzpath = Path('/clustering.npz').expanduser()\n",
    "    \n",
    "    if npzpath.exists():\n",
    "        print(\"Documents have been cached, retrieving now.\")\n",
    "        loaded = np.load(npzpath.as_posix(), allow_pickle=False)\n",
    "        dm = loaded['dist_matrix']\n",
    "        ids = loaded['doc_ids'].tolist()\n",
    "    \n",
    "    else:\n",
    "        print(\"Documents are not cached, building cache now.\")\n",
    "        # db = utils.connect()\n",
    "        allDocuments = utils.getAllDocuments(db, projection={'id':1, 'textVector':1, '_id':0}, batching=True, batchSize=10000)\n",
    "        ids = [x['id'] for x in allDocuments]\n",
    "        print(f'There are {len(ids)} ids in documents.')\n",
    "\n",
    "        vecs = np.array([x['textVector'] for x in allDocuments])\n",
    "        dm = euclidean_distances(vecs)\n",
    "        print(f'The distance matrix has shape: {dm.shape}')\n",
    "\n",
    "        np.savez(npzpath.as_posix(), dist_matrix=dm, doc_ids=ids)\n",
    "    \n",
    "    return dm, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016c96f-5296-4ce3-9ec0-c61dd5da3750",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c103ebf-2aab-43b3-859f-e05289a23da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using First Group's Teleoscope Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e80c898-d93b-4e6b-8e82-195032e46b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = np.load('teleo_order_docs.npz')\n",
    "document_ids = loaded['doc_ids'].tolist()\n",
    "document_vectors = loaded['doc_vecs']\n",
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424b737-865f-40b5-9578-481ca20a91d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using Entire Set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7214856-e392-4503-8c29-bfba1ed0ac12",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "loaded = np.load('all_order_docs.npz')\n",
    "document_ids = loaded['doc_ids'].tolist()\n",
    "document_vectors = loaded['doc_vecs']\n",
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b78ab-60ad-49a3-8fa7-4b9de0b109eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Append documents in human clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c475f795-f600-4618-b95e-a80caf02b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding group 0\n",
      "Document ids has the shape:  10000\n",
      "Document vectors has the shape:  (10000, 512)\n",
      "\n",
      "Adding group 1\n",
      "Document ids has the shape:  10005\n",
      "Document vectors has the shape:  (10005, 512)\n",
      "\n",
      "Adding group 2\n",
      "Document ids has the shape:  10010\n",
      "Document vectors has the shape:  (10010, 512)\n",
      "{'wifi': [5630, 7789, 2801, 3965], 'password': [10000, 10001, 10002, 10003, 6135, 9393, 10004], 'security': [10005, 10006, 10007, 10008, 10009]}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "group_doc_indices = {}\n",
    "for group in groups:\n",
    "    \n",
    "    # grab latest history item for each group\n",
    "    group_document_ids = group[\"history\"][0][\"included_documents\"]\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for id in group_document_ids:\n",
    "        \n",
    "        try:\n",
    "            document_ids.index(id)\n",
    "        \n",
    "        except:\n",
    "            document = db.documents.find_one({\"id\": id}, projection=projection)\n",
    "            document_ids.append(id)\n",
    "            vector = np.array(document[\"textVector\"]).reshape((1, 512))\n",
    "            document_vectors = np.append(document_vectors, vector, axis=0)\n",
    "            \n",
    "        finally:\n",
    "            indices.append(document_ids.index(id))\n",
    "    \n",
    "    group_doc_indices[group[\"history\"][0][\"label\"]] = indices\n",
    "    \n",
    "    print(f'\\nAdding group {i}')\n",
    "    print(\"Document ids has the shape: \", len(document_ids))\n",
    "    print(\"Document vectors has the shape: \", document_vectors.shape)\n",
    "\n",
    "            \n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(group_doc_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2a7fd-49f4-4880-94e0-68a47c16ccb3",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a11868-4892-426a-be99-3f87806c185b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8128d-9cc6-4c7f-88bb-2057d46160f6",
   "metadata": {},
   "source": [
    "##### Create Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c016a88f-8909-4efb-8f42-8df9bbe9b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 10010)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklean euclidean distances\n",
    "dm = euclidean_distances(document_vectors)\n",
    "dm.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be0a3a62-5f26-42cf-a16b-95a8e0582738",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "loaded = np.load('all_order_docs.npz')\n",
    "document_vectors = loaded['doc_vecs']\n",
    "dm = euclidean_distances(document_vectors)\n",
    "np.savez_compressed('all_docs_dm', dist_mat=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705b26c-aa08-4a3e-9c93-af7e88549128",
   "metadata": {},
   "source": [
    "##### Map 0 Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3bbb54-c5bc-4377-addf-1a150823d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wifi': [5630, 7789, 2801, 3965],\n",
       " 'password': [10000, 10001, 10002, 10003, 6135, 9393, 10004],\n",
       " 'security': [10005, 10006, 10007, 10008, 10009]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_doc_indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce6cf17d-e978-4081-9823-99a31e10791b",
   "metadata": {},
   "source": [
    "for group in range(len(groups)):\n",
    "    docs = groups[group]['history'][0]['included_documents']\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        index_i = document_ids.index(docs[i])\n",
    "\n",
    "        for j in range(len(docs)):\n",
    "            index_j = document_ids.index(docs[j])\n",
    "            dm[index_i, index_j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a6b556-e95f-4c53-bb57-33b35071c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in group_doc_indices:\n",
    "    \n",
    "    indices = group_doc_indices[group]\n",
    "    size = range(len(indices))\n",
    "\n",
    "    for _i in size:\n",
    "        i = indices[_i]\n",
    "\n",
    "        for _j in size:\n",
    "            j = indices[_j]\n",
    "            dm[i, j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe95e60-a7af-44a3-a710-d84cdb504bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check to make sure two docs in the same human cluster are distance 0\n",
    "i = group_doc_indices['password'][0]\n",
    "j = group_doc_indices['password'][3]\n",
    "dm[i,j] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21407661-f656-4b12-8ea8-185de17acd27",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a243179c-5c48-4e89-a056-75db8213bf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leofk/opt/anaconda3/envs/mallard/lib/python3.10/site-packages/umap/umap_.py:1780: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(metric='precomputed', min_dist=0.0, n_components=30, verbose=True)\n",
      "Thu Feb 23 14:16:55 2023 Construct fuzzy simplicial set\n",
      "Thu Feb 23 14:16:55 2023 Finding Nearest Neighbors\n",
      "Thu Feb 23 14:16:56 2023 Finished Nearest Neighbor Search\n",
      "Thu Feb 23 14:16:56 2023 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22baff3bb04c6da30da8851dc707ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 23 14:17:00 2023 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "umap_embeddings = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 30,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit_transform(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "df1e5cf5-97ff-40c6-a4cf-4e1e3b368c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 30)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b21ff3b-9ea6-4a7b-8109-eaf000f1c53a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# compute inter cluster distances for human groups\n",
    "for group in range(len(groups)):\n",
    "    \n",
    "    docs = groups[group]['history'][0]['included_documents']\n",
    "    print(f'Group {group}')\n",
    "    dst = []\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        \n",
    "        index_i = document_ids.index(docs[i])\n",
    "        a = umap_embeddings[index_i]\n",
    "        \n",
    "        for j in range(len(docs)):\n",
    "        \n",
    "            index_j = document_ids.index(docs[j])\n",
    "            b = umap_embeddings[index_j]\n",
    "            euc_a_b = euclidean_distances([a],[b])[0][0]\n",
    "            dst.append(euc_a_b)\n",
    "            # print(f'dist({index_i}, {index_j}) = {euc_a_b}')\n",
    "    \n",
    "    mean = sum(dst) / len(dst)\n",
    "    print(f'AVG Dist = {mean}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854923fe-c85f-4224-a1bc-18e0fd5df75a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "117b726a-f2cc-4589-9c0e-d1da1f495d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Clusters = 44 + outliers\n"
     ]
    }
   ],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(\n",
    "    min_cluster_size = 10,              # n-neighbors needed to be considered a cluster (0:50 df=5)\n",
    "    # min_samples = 5,                  # how conservative clustering will be, larger is more conservative (more outliers) (df=None)\n",
    "    cluster_selection_epsilon = 0.2,    # have large clusters in dense regions while leaving smaller clusters small\n",
    "                                        # merge clusters if inter cluster distance is less than thres (df=0)\n",
    ").fit_predict(umap_embeddings)\n",
    "\n",
    "print(f'Num Clusters = {max(hdbscan_labels)+1} + outliers')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0cab874-a831-4a8f-a490-73aec661c46f",
   "metadata": {},
   "source": [
    "# kmeans for fun \n",
    "kmeans_labels = KMeans(n_clusters=10).fit(umap_embeddings).labels_\n",
    "kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ddd2470e-4f42-45c8-90d3-e8dfff440974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels for group = wifi\n",
      "\n",
      "hetv62 42\n",
      "lwd55z 42\n",
      "dhbdpv 42\n",
      "eyj0sv 42\n",
      "\n",
      "Labels for group = password\n",
      "\n",
      "g3y7dc 42\n",
      "j8nzf5 42\n",
      "fs0vuw 42\n",
      "q9zlgr 42\n",
      "ia4w5v 42\n",
      "ruuxs1 42\n",
      "hw16a9 42\n",
      "\n",
      "Labels for group = security\n",
      "\n",
      "mnqbp9 42\n",
      "spk73c 42\n",
      "qqwzth 42\n",
      "dfon3v 42\n",
      "bqafew 42\n"
     ]
    }
   ],
   "source": [
    "# examine matchings between human labelled clusters and machine labelled clusters\n",
    "for group in group_doc_indices:\n",
    "    print(f'\\nLabels for group = {group}\\n')\n",
    "    for index in group_doc_indices[group]:\n",
    "        print(document_ids[index], hdbscan_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655a679-ee5d-4707-b7b4-cd8b968dfa5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results\n",
    "Are human clusters maintained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "91d735b1-5ddf-4be5-a8d5-8c7053339c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wifi': 42, 'password': 42, 'security': 42}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_labels = {}\n",
    "\n",
    "for group in group_doc_indices:\n",
    "    \n",
    "    labels = hdbscan_labels[group_doc_indices[group]] \n",
    "    correct_label = max(labels)\n",
    "    \n",
    "    if -1 in labels:\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == -1:\n",
    "                index = group_doc_indices[group][i]\n",
    "                hdbscan_labels[index] = correct_label\n",
    "    \n",
    "    given_labels[group] = correct_label\n",
    "               \n",
    "given_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a6103451-9b74-4b13-9f0a-a364fc8eac24",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Collection' object is not callable. If you meant to call the 'countDocuments' method on a 'Collection' object it is failing because no such method exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [239], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m user_id \u001b[38;5;241m=\u001b[39m ObjectId(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m63868b5fb3cde877de34970d\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# user: leo\t\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# list(db.groups.find({\"history.user\": user_id}))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountDocuments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhistory.user\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mObjectId\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m63868b5fb3cde877de34970d\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mallard/lib/python3.10/site-packages/pymongo/collection.py:3213\u001b[0m, in \u001b[0;36mCollection.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name:\n\u001b[1;32m   3207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollection\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not callable. If you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeant to call the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method on a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatabase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3210\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject it is failing because no such method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexists.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\n\u001b[1;32m   3212\u001b[0m     )\n\u001b[0;32m-> 3213\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollection\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not callable. If you meant to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method on a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollection\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object it is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3216\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailing because no such method exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3217\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Collection' object is not callable. If you meant to call the 'countDocuments' method on a 'Collection' object it is failing because no such method exists."
     ]
    }
   ],
   "source": [
    "user_id = ObjectId('63868b5fb3cde877de34970d') # user: leo\t\n",
    "# list(db.groups.find({\"history.user\": user_id}))\n",
    "db.groups.countDocuments( {\"history.user\": ObjectId('63868b5fb3cde877de34970d')}, { limit: 1 } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "485557c8-5ef2-473d-841b-c114f31ee050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(hdbscan_label, given_labels):\n",
    "    \"\"\"\n",
    "    if -1:              label = 'outliers'   color = #700c1d\n",
    "    if human cluster:   label = human label  color = human color? or #15540d\n",
    "    if machine cluster: label = topic guess  color = #737373\n",
    "\n",
    "    \"\"\"\n",
    "    check = more = False\n",
    "    \n",
    "    if hdbscan_label == -1:\n",
    "        return 'outliers', '#700c1d'\n",
    "\n",
    "    for _name in given_labels:\n",
    "\n",
    "        label = given_labels[_name]\n",
    "        \n",
    "        if (hdbscan_label == label):\n",
    "            if more:\n",
    "                name += \" & \" + _name\n",
    "            else:\n",
    "                name = _name\n",
    "                more = check = True\n",
    "    \n",
    "    if check:\n",
    "        return name, '#15540d'\n",
    "\n",
    "    return 'machine', '#737373'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5ff67f2b-e76c-4129-aee9-72754122d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5b2c020b-ac95-454b-af14-278c2074ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(label_ids):\n",
    "    \n",
    "    docs = [] \n",
    "    \n",
    "    label_ids = label_ids.tolist()\n",
    "    cursor = db.documents.find({\"id\":{\"$in\": label_ids}})\n",
    "\n",
    "    for document in tqdm.tqdm(cursor):\n",
    "        docs.append(document[\"text\"])\n",
    "        \n",
    "    docs_pp = [preprocess(text) for text in nlp.pipe(docs)]\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    X = vec.fit_transform(docs_pp)\n",
    "\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=1, learning_method=\"batch\", max_iter=10\n",
    "    )\n",
    "    \n",
    "    document_topics = lda.fit_transform(X)\n",
    "    sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    \n",
    "    return feature_names[sorting[0][0]] + \" \" + feature_names[sorting[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d910e6b2-59e8-48aa-83db-a30e6f805095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Dr. Varada Kolhatkar adapted from cpsc330\n",
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "43884d33-3826-4188-bdd0-9acdc143e543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 61.19it/s]\n",
      "20it [00:00, 75.82it/s]\n",
      "12it [00:00, 73.32it/s]\n",
      "20it [00:00, 81.63it/s]\n",
      "20it [00:00, 77.68it/s]\n",
      "20it [00:00, 77.48it/s]\n",
      "19it [00:00, 78.40it/s]\n",
      "14it [00:00, 89.19it/s]\n",
      "20it [00:00, 82.12it/s]\n",
      "20it [00:00, 65.54it/s]\n",
      "20it [00:00, 74.39it/s]\n",
      "20it [00:00, 81.64it/s]\n",
      "18it [00:00, 73.26it/s]\n",
      "17it [00:00, 70.39it/s]\n",
      "20it [00:00, 78.01it/s]\n",
      "17it [00:00, 70.71it/s]\n",
      "16it [00:00, 69.70it/s]\n",
      "20it [00:00, 57.49it/s]\n",
      "20it [00:00, 51.90it/s]\n",
      "20it [00:00, 73.17it/s]\n",
      "12it [00:00, 73.06it/s]\n",
      "14it [00:00, 81.32it/s]\n",
      "20it [00:00, 81.43it/s]\n",
      "14it [00:00, 75.98it/s]\n",
      "20it [00:00, 77.89it/s]\n",
      "20it [00:00, 85.21it/s]\n",
      "13it [00:00, 58.49it/s]\n",
      "20it [00:00, 78.64it/s]\n",
      "20it [00:00, 96.75it/s]\n",
      "13it [00:00, 78.86it/s]\n",
      "20it [00:00, 87.25it/s]\n",
      "20it [00:00, 98.50it/s]\n",
      "20it [00:00, 108.96it/s]\n",
      "20it [00:00, 79.00it/s]\n",
      "20it [00:00, 104.61it/s]\n",
      "20it [00:00, 38.81it/s]\n",
      "20it [00:00, 118.08it/s]\n",
      "18it [00:00, 103.12it/s]\n",
      "20it [00:00, 114.14it/s]\n",
      "10it [00:00, 106.17it/s]\n",
      "20it [00:00, 104.36it/s]\n",
      "20it [00:00, 103.08it/s]\n",
      "17it [00:00, 94.06it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "\n",
    "for hdbscan_label in set(hdbscan_labels):\n",
    "        \n",
    "        # array of indices of documents with current hdbscan label\n",
    "        document_indices_array = np.where(hdbscan_labels == hdbscan_label)[0]\n",
    "        \n",
    "        # all document_ids as array\n",
    "        ids = np.array(document_ids)\n",
    "        \n",
    "        # array of ids of documents with current hdbscan label \n",
    "        label_ids = ids[document_indices_array]\n",
    "\n",
    "        # create list of document ids that are in current hdbscan label\n",
    "        documents = label_ids.tolist()\n",
    "        \n",
    "        # create appropriate label for current hdbscan label\n",
    "        _label, _color = get_label(hdbscan_label, given_labels)\n",
    "        \n",
    "        # learn a topic label for machine clusters\n",
    "        if _label == 'machine':\n",
    "            limit = min(20, len(label_ids))\n",
    "            _label = get_topic(label_ids[:limit])\n",
    "        \n",
    "        # add label and respective document ids to clusters dictionary\n",
    "        clusters[_label] = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "62ea6155-58ca-4df7-8603-f50b0ee73323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['car parking', 'dog time', 'gym workout', 'smell smoke', 'smell room', 'work coworker', 'speed car', 'bus stop', 'drive work', 'adhd tell', 'cat roommate', 'door lock', 'work day', 'stream game', 'room roommate', 'watch want', 'brother tell', 'roommate room', 'food roommate', 'roommate dish', 'game play', 'friend tell', 'play game', 'play ball', 'class work', 'friend play', 'pay tell', 'time food', 'computer laptop', 'work time', 'tell brother', 'room brother', 'job time', 'roommate friend', 'music time', 'work music', 'room music', 'friend roommate', 'sleep night', 'room noise', 'sleep room', 'wifi & password & security', 'outliers'])\n"
     ]
    }
   ],
   "source": [
    "print(clusters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dac6068f-de82-4218-aeb7-115fd8b9f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7f2ed-ff3c-4834-9d13-af0877255cdf",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7115fe00-d884-42ce-958f-8dbfd8119806",
   "metadata": {},
   "source": [
    "mapper = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    # metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 2,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit(umap_embeddings)\n",
    "twod_umap = mapper.transform(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ced8a1-d0a5-43e2-8710-63c8ae76c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "twod_umap = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 2,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit_transform(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648d100-77bb-421a-9009-c8d161365b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,            # num of neighbouring points needed to be considered a cluster\n",
    "    min_samples=None,               # how conservative clustering will be. larger is more conservative.\n",
    "    cluster_selection_epsilon=0.2,   # what it means for points to be “close”\n",
    ").fit(twod_umap)\n",
    "\n",
    "# hdbscan_labels = hdbscan.HDBSCAN(\n",
    "#     min_cluster_size=10,            # num of neighbouring points needed to be considered a cluster\n",
    "#     min_samples=None,               # how conservative clustering will be. larger is more conservative.\n",
    "#     cluster_selection_epsilon=0.2,   # what it means for points to be “close”\n",
    "# ).fit_predict(twod_umap)\n",
    "\n",
    "print(f'Num Clusters = {max(hdbscan_labels)+1} + outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920224af-f335-451b-a201-45861d569a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d4a32-4402-47fc-ba72-620de17e0713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap.plot.points(mapper, labels=hdbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77ebfe-bc1e-45db-a023-da87aeaf51dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mallard]",
   "language": "python",
   "name": "conda-env-mallard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "873bf3bf4ca629f02780c1468f98ab03c421afd8de10b4352774f82eeed6b96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
