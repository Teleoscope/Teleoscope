{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d979288c-7e14-4d92-b9de-6c2acc9571ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84f9d1b9-d81c-4c83-9c9d-30d03eab859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from bson.objectid import ObjectId\n",
    "import gc\n",
    "import tasks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numba\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, cosine_distances\n",
    "import umap.plot\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# for jupyter notebook widgets\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac8027c-af0d-4bf3-8d06-176ced95da0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['20.220.215.35:27017'], document_class=dict, tz_aware=False, connect=True, authsource='admin', authmechanism='SCRAM-SHA-256', connecttimeoutms=50000, serverselectiontimeoutms=50000, directconnection=True, replicaset='rs0'), 'aita')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to database\n",
    "db = utils.connect()\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe4861-8e82-4434-b536-fcf9541d0648",
   "metadata": {},
   "source": [
    "### Setup Human Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4448d3-9fca-44af-a4df-16718f686a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 groups from database\n"
     ]
    }
   ],
   "source": [
    "# hardcoded group id strings\n",
    "group_id_strings = ['63901a89e189962b660959cf', '63901a92931eeac91c9924a1', '63901a96e189962b660959d3']\n",
    "\n",
    "# convert to objectId's\n",
    "group_ids = [ObjectId(str(id)) for id in group_id_strings]\n",
    "\n",
    "# retrieve groups from database\n",
    "groups = list(db.groups.find({\"_id\":{\"$in\" : group_ids}}))\n",
    "print(\"Retrieved \" + str(len(groups)) + \" groups from database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8244c486-26d3-409d-b914-30eaec8a44a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hetv62', 'lwd55z', 'dhbdpv', 'eyj0sv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups[0]['history'][0]['included_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c27a74c-e029-40a5-b0ee-c69cd123b4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1, 'textVector': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projection here to only include the fields we want\n",
    "projection = {'id': 1, 'textVector': 1}\n",
    "projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74e643-031e-4e66-a2d1-ba5c04f935df",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54c336-714e-42b1-94b5-24fe2b2f707e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Save & Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415165c7-d80c-4728-b3e2-f6659abe0808",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using First Group's Teleoscope Ordering\n",
    "\n",
    "Change Raw cells below to Code if you need to reload document ids / vectors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0eabfa6c-243a-4e36-ae65-f2af0e9a75a4",
   "metadata": {},
   "source": [
    "# default to ordering documents relative to first group's teleoscope\n",
    "teleoscope_oid = groups[0][\"teleoscope\"]\n",
    "teleoscope_oid"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b83972eb-2932-4801-9936-9353a94fc786",
   "metadata": {
    "tags": []
   },
   "source": [
    "teleoscope = db.teleoscopes.find_one({\"_id\": ObjectId(str(teleoscope_oid))})\n",
    "#teleoscope"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b3894a6-20b1-4156-9fb7-fc9b15382bc1",
   "metadata": {},
   "source": [
    "# saved as ordered_documents.npz\n",
    "# change to code cell if load is needed\n",
    "\n",
    "# get Teleoscope from GridFS\n",
    "all_ordered_documents = utils.gridfsDownload(db, \"teleoscopes\", ObjectId(str(teleoscope[\"history\"][0][\"ranked_document_ids\"])))\n",
    "\n",
    "# np.savez_compressed('all_ordered_documents', ord_docs=all_ordered_documents)\n",
    "# all_ordered_documents = np.load('all_ordered_documents.npz')['ord_docs']\n",
    "\n",
    "len(all_ordered_documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdab68b3-ba24-415c-a01b-7bf11fdddac9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# grab only subset of the ordered documents\n",
    "limit = 10000\n",
    "# TODO: does this line generate an out of bounds access?\n",
    "ordered_documents = all_ordered_documents[0:limit]\n",
    "limit = min(limit, len(ordered_documents))\n",
    "limit"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ff88c0d-2f74-4434-8805-a5d8bf771a38",
   "metadata": {},
   "source": [
    "ordered_documents = all_ordered_documents\n",
    "limit = len(ordered_documents)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab1f7ac9-9c38-4273-bfb9-f2e0933df270",
   "metadata": {},
   "source": [
    "# cursor is a generator which means it yields a new doc one at a time\n",
    "cursor = db.documents.find(\n",
    "    # query\n",
    "    {\"id\":{\"$in\": [document[0] for document in ordered_documents]}},\n",
    "    projection=projection,\n",
    "    # batch size means number of documents at a time taken from MDB, no impact on iteration\n",
    "    batch_size=500\n",
    ")\n",
    "document_ids = []\n",
    "document_vectors = []\n",
    "\n",
    "# for large datasets, this will take a while. Would be better to find out whether the UMAP fns can \n",
    "# accept generators for lazy calculation\n",
    "for document in tqdm.tqdm(cursor, total=limit):\n",
    "    document_ids.append(document[\"id\"])\n",
    "    document_vectors.append(document[\"textVector\"])\n",
    "\n",
    "print(\"There are \" + str(len(document_ids)) + \" document ids.\")\n",
    "print(\"There are \" + str(len(document_vectors)) + \" document vectors.\")\n",
    "\n",
    "np.savez_compressed('teleo_order_docs', doc_ids=document_ids, doc_vecs=document_vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975b6eb-51c0-408b-9232-5967aba84cd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using All Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3fd1390-d60c-4ae8-b139-248acdec6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacheClusteringData(db):\n",
    "    \"\"\"\n",
    "    Check to see if distance matrix and list of document ids is cached in ~/embeddings\n",
    "    \n",
    "    input:\n",
    "        db: mongoDB connection\n",
    "    output:\n",
    "        dm: distance matrix\n",
    "        ids: list of document ids\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    npzpath = Path('/clustering.npz').expanduser()\n",
    "    \n",
    "    if npzpath.exists():\n",
    "        print(\"Documents have been cached, retrieving now.\")\n",
    "        loaded = np.load(npzpath.as_posix(), allow_pickle=False)\n",
    "        dm = loaded['dist_matrix']\n",
    "        ids = loaded['doc_ids'].tolist()\n",
    "    \n",
    "    else:\n",
    "        print(\"Documents are not cached, building cache now.\")\n",
    "        # db = utils.connect()\n",
    "        allDocuments = utils.getAllDocuments(db, projection={'id':1, 'textVector':1, '_id':0}, batching=True, batchSize=10000)\n",
    "        ids = [x['id'] for x in allDocuments]\n",
    "        print(f'There are {len(ids)} ids in documents.')\n",
    "\n",
    "        vecs = np.array([x['textVector'] for x in allDocuments])\n",
    "        dm = euclidean_distances(vecs)\n",
    "        print(f'The distance matrix has shape: {dm.shape}')\n",
    "\n",
    "        np.savez(npzpath.as_posix(), dist_matrix=dm, doc_ids=ids)\n",
    "    \n",
    "    return dm, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0478269-29ba-494b-85b4-4cae9884c2d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using average teleoscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d74bc94-285e-4d17-a04d-f2d834d5db05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347807"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this would actually be from ~/embeddings\n",
    "loaded = np.load('all_order_docs.npz')\n",
    "document_ids = loaded['doc_ids'].tolist()\n",
    "document_vectors = loaded['doc_vecs']\n",
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "988bd1df-b9f6-41fe-970a-44f8750e8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = min(10000, len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e99d1e2a-aadc-4c74-9405-71e5c8e61751",
   "metadata": {},
   "outputs": [],
   "source": [
    "teleoscope_oid = groups[0][\"teleoscope\"]\n",
    "teleoscope = db.teleoscopes.find_one({\"_id\": ObjectId(str(teleoscope_oid))})\n",
    "all_ordered_documents = utils.gridfsDownload(db, \"teleoscopes\", ObjectId(str(teleoscope[\"history\"][0][\"ranked_document_ids\"])))\n",
    "ordered_documents = all_ordered_documents[0:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a866b21-b803-4e0e-babb-ccd3d38b2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hetv62', 1.0000000479605118],\n",
       " ['gg73t6', 0.7192117139308498],\n",
       " ['ssc1sq', 0.7128566050901863],\n",
       " ['tdtext', 0.712335848173534],\n",
       " ['ro4lqk', 0.703690002226738]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5912257c-5b35-4309-8348-0f521ff9836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347807, 512)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91f23b32-22c9-462c-a31f-4c60ac9a0451",
   "metadata": {},
   "outputs": [],
   "source": [
    "teleo_vecs = []\n",
    "for group in groups:\n",
    "\n",
    "    teleoscope_oid = group[\"teleoscope\"]\n",
    "    teleoscope = db.teleoscopes.find_one({\"_id\": ObjectId(str(teleoscope_oid))})\n",
    "    teleo_vecs.append(teleoscope[\"history\"][0][\"stateVector\"])\n",
    "\n",
    "teleo_vecs = np.array(teleo_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f66a4496-001a-40af-b987-aae1d181a10b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teleo_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "16095cdf-f805-4c3b-adc4-359e7a294634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vec = np.average(teleo_vecs, axis=0)\n",
    "avg_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36dd8f68-a92f-4712-b4ce-b9d3a6245ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23058389, 0.31979471, 0.30451099, 0.30814943, 0.29490711])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = utils.calculateSimilarity(document_vectors, avg_vec)\n",
    "scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a21e357c-c65d-40d4-a548-f793e54ef28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ia4w5v', 'flo65r', 'sgt76q', 'fzuf7q', 'bqafew']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = utils.rankDocumentsBySimilarity(document_ids, scores)[:limit]\n",
    "ids = [i for i, j in ids]\n",
    "ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ede449e1-18f7-4e2c-959f-5fe06d32683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = {'id': 1, 'textVector': 1}\n",
    "cursor = db.documents.find(\n",
    "    # query\n",
    "    {\"id\":{\"$in\": [i for i in ids]}},\n",
    "    projection=projection,\n",
    "    # batch size means number of documents at a time taken from MDB, no impact on iteration \n",
    "    batch_size=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba4a4391-3d71-48b9-b007-838e44f89282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "document_ids = []\n",
    "document_vectors = []\n",
    "\n",
    "# for large datasets, this will take a while. Would be better to find out whether the UMAP fns \n",
    "# can accept generators for lazy calculation \n",
    "for document in tqdm.tqdm(cursor, total=limit):\n",
    "    document_ids.append(document[\"id\"])\n",
    "    document_vectors.append(document[\"textVector\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016c96f-5296-4ce3-9ec0-c61dd5da3750",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c103ebf-2aab-43b3-859f-e05289a23da8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using First Group's Teleoscope Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e80c898-d93b-4e6b-8e82-195032e46b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = np.load('teleo_order_docs.npz')\n",
    "document_ids = loaded['doc_ids'].tolist()\n",
    "document_vectors = loaded['doc_vecs']\n",
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8424b737-865f-40b5-9578-481ca20a91d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Using Entire Set"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7214856-e392-4503-8c29-bfba1ed0ac12",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "loaded = np.load('all_order_docs.npz')\n",
    "document_ids = loaded['doc_ids'].tolist()\n",
    "document_vectors = loaded['doc_vecs']\n",
    "len(document_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b78ab-60ad-49a3-8fa7-4b9de0b109eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Append documents in human clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c475f795-f600-4618-b95e-a80caf02b683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding group 0\n",
      "Document ids has the shape:  10000\n",
      "Document vectors has the shape:  (10000, 512)\n",
      "\n",
      "Adding group 1\n",
      "Document ids has the shape:  10005\n",
      "Document vectors has the shape:  (10005, 512)\n",
      "\n",
      "Adding group 2\n",
      "Document ids has the shape:  10010\n",
      "Document vectors has the shape:  (10010, 512)\n",
      "{'wifi': [5630, 7789, 2801, 3965], 'password': [10000, 10001, 10002, 10003, 6135, 9393, 10004], 'security': [10005, 10006, 10007, 10008, 10009]}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "group_doc_indices = {}\n",
    "for group in groups:\n",
    "    \n",
    "    # grab latest history item for each group\n",
    "    group_document_ids = group[\"history\"][0][\"included_documents\"]\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    for id in group_document_ids:\n",
    "        \n",
    "        try:\n",
    "            document_ids.index(id)\n",
    "        \n",
    "        except:\n",
    "            document = db.documents.find_one({\"id\": id}, projection=projection)\n",
    "            document_ids.append(id)\n",
    "            vector = np.array(document[\"textVector\"]).reshape((1, 512))\n",
    "            document_vectors = np.append(document_vectors, vector, axis=0)\n",
    "            \n",
    "        finally:\n",
    "            indices.append(document_ids.index(id))\n",
    "    \n",
    "    group_doc_indices[group[\"history\"][0][\"label\"]] = indices\n",
    "    \n",
    "    print(f'\\nAdding group {i}')\n",
    "    print(\"Document ids has the shape: \", len(document_ids))\n",
    "    print(\"Document vectors has the shape: \", document_vectors.shape)\n",
    "\n",
    "            \n",
    "\n",
    "    i += 1\n",
    "\n",
    "print(group_doc_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2a7fd-49f4-4880-94e0-68a47c16ccb3",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a11868-4892-426a-be99-3f87806c185b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8128d-9cc6-4c7f-88bb-2057d46160f6",
   "metadata": {},
   "source": [
    "##### Create Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c016a88f-8909-4efb-8f42-8df9bbe9b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 10010)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sklean euclidean distances\n",
    "dm = euclidean_distances(document_vectors)\n",
    "dm.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be0a3a62-5f26-42cf-a16b-95a8e0582738",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "loaded = np.load('all_order_docs.npz')\n",
    "document_vectors = loaded['doc_vecs']\n",
    "dm = euclidean_distances(document_vectors)\n",
    "np.savez_compressed('all_docs_dm', dist_mat=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705b26c-aa08-4a3e-9c93-af7e88549128",
   "metadata": {},
   "source": [
    "##### Map 0 Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3bbb54-c5bc-4377-addf-1a150823d682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wifi': [5630, 7789, 2801, 3965],\n",
       " 'password': [10000, 10001, 10002, 10003, 6135, 9393, 10004],\n",
       " 'security': [10005, 10006, 10007, 10008, 10009]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_doc_indices"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce6cf17d-e978-4081-9823-99a31e10791b",
   "metadata": {},
   "source": [
    "for group in range(len(groups)):\n",
    "    docs = groups[group]['history'][0]['included_documents']\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        index_i = document_ids.index(docs[i])\n",
    "\n",
    "        for j in range(len(docs)):\n",
    "            index_j = document_ids.index(docs[j])\n",
    "            dm[index_i, index_j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a6b556-e95f-4c53-bb57-33b35071c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in group_doc_indices:\n",
    "    \n",
    "    indices = group_doc_indices[group]\n",
    "    size = range(len(indices))\n",
    "\n",
    "    for _i in size:\n",
    "        i = indices[_i]\n",
    "\n",
    "        for _j in size:\n",
    "            j = indices[_j]\n",
    "            dm[i, j] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe95e60-a7af-44a3-a710-d84cdb504bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check to make sure two docs in the same human cluster are distance 0\n",
    "i = group_doc_indices['password'][0]\n",
    "j = group_doc_indices['password'][3]\n",
    "dm[i,j] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21407661-f656-4b12-8ea8-185de17acd27",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a243179c-5c48-4e89-a056-75db8213bf01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leofk/opt/anaconda3/envs/mallard/lib/python3.10/site-packages/umap/umap_.py:1780: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(metric='precomputed', min_dist=0.0, n_components=30, verbose=True)\n",
      "Tue Feb 28 11:11:50 2023 Construct fuzzy simplicial set\n",
      "Tue Feb 28 11:11:50 2023 Finding Nearest Neighbors\n",
      "Tue Feb 28 11:11:52 2023 Finished Nearest Neighbor Search\n",
      "Tue Feb 28 11:11:55 2023 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8055a5407a0348c78b70e4d01d0499d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 28 11:11:59 2023 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "umap_embeddings = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 30,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit_transform(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df1e5cf5-97ff-40c6-a4cf-4e1e3b368c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10010, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b21ff3b-9ea6-4a7b-8109-eaf000f1c53a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# compute inter cluster distances for human groups\n",
    "for group in range(len(groups)):\n",
    "    \n",
    "    docs = groups[group]['history'][0]['included_documents']\n",
    "    print(f'Group {group}')\n",
    "    dst = []\n",
    "    \n",
    "    for i in range(len(docs)):\n",
    "        \n",
    "        index_i = document_ids.index(docs[i])\n",
    "        a = umap_embeddings[index_i]\n",
    "        \n",
    "        for j in range(len(docs)):\n",
    "        \n",
    "            index_j = document_ids.index(docs[j])\n",
    "            b = umap_embeddings[index_j]\n",
    "            euc_a_b = euclidean_distances([a],[b])[0][0]\n",
    "            dst.append(euc_a_b)\n",
    "            # print(f'dist({index_i}, {index_j}) = {euc_a_b}')\n",
    "    \n",
    "    mean = sum(dst) / len(dst)\n",
    "    print(f'AVG Dist = {mean}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854923fe-c85f-4224-a1bc-18e0fd5df75a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117b726a-f2cc-4589-9c0e-d1da1f495d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Clusters = 44 + outliers\n"
     ]
    }
   ],
   "source": [
    "hdbscan_labels = hdbscan.HDBSCAN(\n",
    "    min_cluster_size = 10,              # n-neighbors needed to be considered a cluster (0:50 df=5)\n",
    "    # min_samples = 5,                  # how conservative clustering will be, larger is more conservative (more outliers) (df=None)\n",
    "    cluster_selection_epsilon = 0.2,    # have large clusters in dense regions while leaving smaller clusters small\n",
    "                                        # merge clusters if inter cluster distance is less than thres (df=0)\n",
    ").fit_predict(umap_embeddings)\n",
    "\n",
    "print(f'Num Clusters = {max(hdbscan_labels)+1} + outliers')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0cab874-a831-4a8f-a490-73aec661c46f",
   "metadata": {},
   "source": [
    "# kmeans for fun \n",
    "kmeans_labels = KMeans(n_clusters=10).fit(umap_embeddings).labels_\n",
    "kmeans_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd2470e-4f42-45c8-90d3-e8dfff440974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels for group = wifi\n",
      "\n",
      "hetv62 40\n",
      "lwd55z 40\n",
      "dhbdpv 40\n",
      "eyj0sv 40\n",
      "\n",
      "Labels for group = password\n",
      "\n",
      "g3y7dc 40\n",
      "j8nzf5 40\n",
      "fs0vuw 40\n",
      "q9zlgr 40\n",
      "ia4w5v 40\n",
      "ruuxs1 40\n",
      "hw16a9 40\n",
      "\n",
      "Labels for group = security\n",
      "\n",
      "mnqbp9 40\n",
      "spk73c 40\n",
      "qqwzth 40\n",
      "dfon3v 40\n",
      "bqafew 40\n"
     ]
    }
   ],
   "source": [
    "# examine matchings between human labelled clusters and machine labelled clusters\n",
    "for group in group_doc_indices:\n",
    "    print(f'\\nLabels for group = {group}\\n')\n",
    "    for index in group_doc_indices[group]:\n",
    "        print(document_ids[index], hdbscan_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b655a679-ee5d-4707-b7b4-cd8b968dfa5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Results\n",
    "Are human clusters maintained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d735b1-5ddf-4be5-a8d5-8c7053339c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wifi': 40, 'password': 40, 'security': 40}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_labels = {}\n",
    "\n",
    "for group in group_doc_indices:\n",
    "    \n",
    "    labels = hdbscan_labels[group_doc_indices[group]] \n",
    "    correct_label = max(labels)\n",
    "    \n",
    "    if -1 in labels:\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] == -1:\n",
    "                index = group_doc_indices[group][i]\n",
    "                hdbscan_labels[index] = correct_label\n",
    "    \n",
    "    given_labels[group] = correct_label\n",
    "               \n",
    "given_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6103451-9b74-4b13-9f0a-a364fc8eac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for user exists. Delete all.\n",
      "No clusters for user. Ready to populate.\n"
     ]
    }
   ],
   "source": [
    "leo = ObjectId('63868b5fb3cde877de34c27d') \n",
    "paul = ObjectId('637ee569d1259b1565f7e97e') \n",
    "session = ObjectId('\"63f68cfba7e87c4253864452\"') \n",
    "\n",
    "userid = leo\n",
    "if db.clusters.count_documents({\"history.user\": userid}, limit=1):\n",
    "    print(f'Clusters for user exists. Delete all.')\n",
    "    db.clusters.delete_many({\"history.user\": userid})\n",
    "print(f'No clusters for user. Ready to populate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "485557c8-5ef2-473d-841b-c114f31ee050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(hdbscan_label, given_labels):\n",
    "    \"\"\"\n",
    "    if -1:              label = 'outliers'   color = #700c1d\n",
    "    if human cluster:   label = human label  color = human color? or #15540d\n",
    "    if machine cluster: label = topic guess  color = #737373\n",
    "\n",
    "    \"\"\"\n",
    "    check = more = False\n",
    "    \n",
    "    if hdbscan_label == -1:\n",
    "        return 'outliers', '#700c1d'\n",
    "\n",
    "    for _name in given_labels:\n",
    "\n",
    "        label = given_labels[_name]\n",
    "        \n",
    "        if (hdbscan_label == label):\n",
    "            if more:\n",
    "                name += \" & \" + _name\n",
    "            else:\n",
    "                name = _name\n",
    "                more = check = True\n",
    "    \n",
    "    if check:\n",
    "        return name, '#15540d'\n",
    "\n",
    "    return 'machine', '#737373'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "5ff67f2b-e76c-4129-aee9-72754122d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5b2c020b-ac95-454b-af14-278c2074ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(label_ids):\n",
    "    \n",
    "    docs = [] \n",
    "    \n",
    "    label_ids = label_ids.tolist()\n",
    "    cursor = db.documents.find({\"id\":{\"$in\": label_ids}})\n",
    "\n",
    "    for document in tqdm.tqdm(cursor):\n",
    "        docs.append(document[\"text\"])\n",
    "        \n",
    "    docs_pp = [preprocess(text) for text in nlp.pipe(docs)]\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    X = vec.fit_transform(docs_pp)\n",
    "\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=1, learning_method=\"batch\", max_iter=10\n",
    "    )\n",
    "    \n",
    "    document_topics = lda.fit_transform(X)\n",
    "    sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "    feature_names = np.array(vec.get_feature_names_out())\n",
    "    \n",
    "    return feature_names[sorting[0][0]] + \" \" + feature_names[sorting[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d910e6b2-59e8-48aa-83db-a30e6f805095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code by Dr. Varada Kolhatkar adapted from cpsc330\n",
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "43884d33-3826-4188-bdd0-9acdc143e543",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 61.19it/s]\n",
      "20it [00:00, 75.82it/s]\n",
      "12it [00:00, 73.32it/s]\n",
      "20it [00:00, 81.63it/s]\n",
      "20it [00:00, 77.68it/s]\n",
      "20it [00:00, 77.48it/s]\n",
      "19it [00:00, 78.40it/s]\n",
      "14it [00:00, 89.19it/s]\n",
      "20it [00:00, 82.12it/s]\n",
      "20it [00:00, 65.54it/s]\n",
      "20it [00:00, 74.39it/s]\n",
      "20it [00:00, 81.64it/s]\n",
      "18it [00:00, 73.26it/s]\n",
      "17it [00:00, 70.39it/s]\n",
      "20it [00:00, 78.01it/s]\n",
      "17it [00:00, 70.71it/s]\n",
      "16it [00:00, 69.70it/s]\n",
      "20it [00:00, 57.49it/s]\n",
      "20it [00:00, 51.90it/s]\n",
      "20it [00:00, 73.17it/s]\n",
      "12it [00:00, 73.06it/s]\n",
      "14it [00:00, 81.32it/s]\n",
      "20it [00:00, 81.43it/s]\n",
      "14it [00:00, 75.98it/s]\n",
      "20it [00:00, 77.89it/s]\n",
      "20it [00:00, 85.21it/s]\n",
      "13it [00:00, 58.49it/s]\n",
      "20it [00:00, 78.64it/s]\n",
      "20it [00:00, 96.75it/s]\n",
      "13it [00:00, 78.86it/s]\n",
      "20it [00:00, 87.25it/s]\n",
      "20it [00:00, 98.50it/s]\n",
      "20it [00:00, 108.96it/s]\n",
      "20it [00:00, 79.00it/s]\n",
      "20it [00:00, 104.61it/s]\n",
      "20it [00:00, 38.81it/s]\n",
      "20it [00:00, 118.08it/s]\n",
      "18it [00:00, 103.12it/s]\n",
      "20it [00:00, 114.14it/s]\n",
      "10it [00:00, 106.17it/s]\n",
      "20it [00:00, 104.36it/s]\n",
      "20it [00:00, 103.08it/s]\n",
      "17it [00:00, 94.06it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters = {}\n",
    "\n",
    "for hdbscan_label in set(hdbscan_labels):\n",
    "        \n",
    "        # array of indices of documents with current hdbscan label\n",
    "        document_indices_array = np.where(hdbscan_labels == hdbscan_label)[0]\n",
    "        \n",
    "        # all document_ids as array\n",
    "        ids = np.array(document_ids)\n",
    "        \n",
    "        # array of ids of documents with current hdbscan label \n",
    "        label_ids = ids[document_indices_array]\n",
    "\n",
    "        # create list of document ids that are in current hdbscan label\n",
    "        documents = label_ids.tolist()\n",
    "        \n",
    "        # create appropriate label for current hdbscan label\n",
    "        _label, _color = get_label(hdbscan_label, given_labels)\n",
    "        \n",
    "        # learn a topic label for machine clusters\n",
    "        if _label == 'machine':\n",
    "            limit = min(20, len(label_ids))\n",
    "            _label = get_topic(label_ids[:limit])\n",
    "        \n",
    "        # add label and respective document ids to clusters dictionary\n",
    "        clusters[_label] = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "62ea6155-58ca-4df7-8603-f50b0ee73323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['car parking', 'dog time', 'gym workout', 'smell smoke', 'smell room', 'work coworker', 'speed car', 'bus stop', 'drive work', 'adhd tell', 'cat roommate', 'door lock', 'work day', 'stream game', 'room roommate', 'watch want', 'brother tell', 'roommate room', 'food roommate', 'roommate dish', 'game play', 'friend tell', 'play game', 'play ball', 'class work', 'friend play', 'pay tell', 'time food', 'computer laptop', 'work time', 'tell brother', 'room brother', 'job time', 'roommate friend', 'music time', 'work music', 'room music', 'friend roommate', 'sleep night', 'room noise', 'sleep room', 'wifi & password & security', 'outliers'])\n"
     ]
    }
   ],
   "source": [
    "print(clusters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "dac6068f-de82-4218-aeb7-115fd8b9f5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7f2ed-ff3c-4834-9d13-af0877255cdf",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7115fe00-d884-42ce-958f-8dbfd8119806",
   "metadata": {},
   "source": [
    "mapper = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    # metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 2,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit(umap_embeddings)\n",
    "twod_umap = mapper.transform(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ced8a1-d0a5-43e2-8710-63c8ae76c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "twod_umap = umap.UMAP(\n",
    "    verbose = True,         # for logging\n",
    "    metric = \"precomputed\", # use distance matrix\n",
    "    n_components = 2,      # reduce to n_components dimensions (2:100)\n",
    "    # n_neighbors = 10,     # local (small n ~2) vs. global (large n ~100) structure \n",
    "    min_dist = 0.0,         # minimum distance apart that points are allowed (0.0:0.99)\n",
    ").fit_transform(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648d100-77bb-421a-9009-c8d161365b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=10,            # num of neighbouring points needed to be considered a cluster\n",
    "    min_samples=None,               # how conservative clustering will be. larger is more conservative.\n",
    "    cluster_selection_epsilon=0.2,   # what it means for points to be “close”\n",
    ").fit(twod_umap)\n",
    "\n",
    "# hdbscan_labels = hdbscan.HDBSCAN(\n",
    "#     min_cluster_size=10,            # num of neighbouring points needed to be considered a cluster\n",
    "#     min_samples=None,               # how conservative clustering will be. larger is more conservative.\n",
    "#     cluster_selection_epsilon=0.2,   # what it means for points to be “close”\n",
    "# ).fit_predict(twod_umap)\n",
    "\n",
    "print(f'Num Clusters = {max(hdbscan_labels)+1} + outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920224af-f335-451b-a201-45861d569a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.single_linkage_tree_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d4a32-4402-47fc-ba72-620de17e0713",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap.plot.points(mapper, labels=hdbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77ebfe-bc1e-45db-a023-da87aeaf51dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mallard] *",
   "language": "python",
   "name": "conda-env-mallard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "873bf3bf4ca629f02780c1468f98ab03c421afd8de10b4352774f82eeed6b96f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
